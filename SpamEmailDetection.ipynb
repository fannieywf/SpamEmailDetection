{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Email Detection\n",
    "## Fannie Yuen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Spams can be detected through natural language processing and machine learning methodologies. \n",
    "Research on spam email detection either focuses on natural language processing methodologies on single machine learning algorithm or one natural language processing technique on multiple machine learning algorithms.\n",
    "In this notebook, a modeling pipeline is developed to review both the natural language processing techniques and the machine learning methodologies. \n",
    "Modeling results show that preprocessing the emails by most-frequent-word-count modeling in logistic regression gives the best model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Email has been used as a modern communication method due to its simplicity and cost-effectiveness. The number of email users worldwide is forecasted to rise to 2.9 billion by 2019. Nearly 105 billion emails are sent each day. This number is expected to reach 246 billion before 2020$^1$. However, spam emails received by recipients are not desirable. Traditional methods of spam filtering such as blacklists and whitelists using mailing addresses, domains, IP addresses are not effective. Alternatively, spam email detection based on machine learning is an application to reduce spam email. \n",
    "\n",
    "In this notebook, a spam email detection classifier will be developed. The following key topics will be covered. First, the data$^2$ will be handled through `email` and `BeautifulSoup` libraries. Text preprocessing with and without **Natural Language Processing (NLP) concerning email content** will be evaluated. A number of **machine learning methodologies** are introduced to detect spam email from the SpamAssassin$^3$, an open source anti-spam platform. Moreover, identification of model **underfitting and overfitting** will be discussed and model performances will be reviewed through cross-validation and evaluation metrics. \n",
    "\n",
    "The development is implemented in **Python 3**. Some codes follow the existing kernel$^4$.\n",
    "\n",
    "The workflow of developing a spam email detection classifier is as follows:\n",
    "\n",
    "1. Data Reading and Inspection\n",
    "2. Text Preprocessing\n",
    "3. Modeling\n",
    "4. Results\n",
    "5. Conclusions\n",
    "6. Improvements\n",
    "7. Appendix\n",
    "\n",
    "Now, let us start with loading the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import email\n",
    "import email.policy\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Reading and Inspection\n",
    "The `email` package manages email message, which loads the email data. The email data consists of either spams or hams. Spams, aka junk emails, are unsolicited messages sent in bulk by email$^5$. Hams are non-spams expected by email recipients. \n",
    "\n",
    "Data is read and inspected according to the existing kernel$^4$. Each file in the data source represents an email message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_filenames = [name for name in sorted(os.listdir('../ham-and-spam-dataset/hamnspam/ham'))]\n",
    "spam_filenames = [name for name in sorted(os.listdir('../ham-and-spam-dataset/hamnspam/spam'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ham files: 2551\n",
      "Amount of spam files: 501\n",
      "Spam to Ham Ratio: 19.64%\n",
      "Spam to All Ratio: 16.42%\n",
      "Ham to All Ratio: 83.58%\n"
     ]
    }
   ],
   "source": [
    "print('Amount of ham files:', len(ham_filenames))\n",
    "print('Amount of spam files:', len(spam_filenames))    \n",
    "print('Spam to Ham Ratio: {:.2f}%'.format(100 * len(spam_filenames) / len(ham_filenames)))\n",
    "print('Spam to All Ratio: {:.2f}%'.format(100 * len(spam_filenames) / (len(ham_filenames) + len(spam_filenames))))\n",
    "print('Ham to All Ratio: {:.2f}%'.format(100 * len(ham_filenames) / (len(ham_filenames) + len(spam_filenames))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 email groups...\n",
      "Emails loaded!\n",
      "CPU times: user 3.52 s, sys: 395 ms, total: 3.92 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_email(is_spam, filename):\n",
    "    directory = \"../ham-and-spam-dataset/hamnspam/spam\" if is_spam else \"../ham-and-spam-dataset/hamnspam/ham\"\n",
    "    with open(os.path.join(directory, filename), \"rb\") as f:\n",
    "        # create BytesParser instance; parse(): parse resulting bytes and return message object\n",
    "        return email.parser.BytesParser(policy = email.policy.default).parse(f)\n",
    "   \n",
    "print(\"Loading 2 email groups...\")\n",
    "ham_emails = [load_email(is_spam = False, filename = name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam = True, filename = name) for name in spam_filenames]\n",
    "print(\"Emails loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the emails, a preview of a sample email is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header Field Names: ['Return-Path', 'Delivered-To', 'Received', 'Received', 'Received', 'X-Egroups-Return', 'Received', 'X-Sender', 'X-Apparently-To', 'Received', 'Received', 'Received', 'Received', 'Received', 'Received', 'Message-Id', 'To', 'X-Mailer', 'X-Egroups-From', 'From', 'X-Yahoo-Profile', 'MIME-Version', 'Mailing-List', 'Delivered-To', 'Precedence', 'List-Unsubscribe', 'Date', 'Subject', 'Reply-To', 'Content-Type', 'Content-Transfer-Encoding']\n",
      "\n",
      "\n",
      "Message Field Values: ['<Steve_Burt@cursor-system.com>', 'zzzz@localhost.netnoteinc.com', 'from localhost (localhost [127.0.0.1])\\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id BE12E43C34\\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:46:38 -0400 (EDT)', 'from phobos [127.0.0.1]\\tby localhost with IMAP (fetchmail-5.9.0)\\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:46:38 +0100 (IST)', 'from n20.grp.scd.yahoo.com (n20.grp.scd.yahoo.com    [66.218.66.76]) by dogma.slashnull.org (8.11.6/8.11.6) with SMTP id    g7MBkTZ05087 for <zzzz@example.com>; Thu, 22 Aug 2002 12:46:29 +0100', 'sentto-2242572-52726-1030016790-zzzz=example.com@returns.groups.yahoo.com', 'from [66.218.67.196] by n20.grp.scd.yahoo.com with NNFMP;    22 Aug 2002 11:46:30 -0000', 'steve.burt@cursor-system.com', 'zzzzteana@yahoogroups.com', '(EGP: mail-8_1_0_1); 22 Aug 2002 11:46:29 -0000', '(qmail 11764 invoked from network); 22 Aug 2002 11:46:29 -0000', 'from unknown (66.218.66.217) by m3.grp.scd.yahoo.com with QMQP;    22 Aug 2002 11:46:29 -0000', 'from unknown (HELO mailgateway.cursor-system.com) (62.189.7.27)    by mta2.grp.scd.yahoo.com with SMTP; 22 Aug 2002 11:46:29 -0000', 'from exchange1.cps.local (unverified) by    mailgateway.cursor-system.com (Content Technologies SMTPRS 4.2.10) with    ESMTP id <T5cde81f695ac1d100407d@mailgateway.cursor-system.com> for    <forteana@yahoogroups.com>; Thu, 22 Aug 2002 13:14:10 +0100', 'by exchange1.cps.local with Internet Mail Service (5.5.2653.19)    id <PXX6AT23>; Thu, 22 Aug 2002 12:46:27 +0100', '<5EC2AD6D2314D14FB64BDA287D25D9EF12B4F6@exchange1.cps.local>', '\"\\'zzzzteana@yahoogroups.com\\'\" <zzzzteana@yahoogroups.com>', 'Internet Mail Service (5.5.2653.19)', 'Steve Burt <steve.burt@cursor-system.com>', 'Steve Burt <Steve_Burt@cursor-system.com>', 'pyruse', '1.0', 'list zzzzteana@yahoogroups.com; contact    forteana-owner@yahoogroups.com', 'mailing list zzzzteana@yahoogroups.com', 'bulk', '<mailto:zzzzteana-unsubscribe@yahoogroups.com>', 'Thu, 22 Aug 2002 12:46:18 +0100', '[zzzzteana] RE: Alexander', 'zzzzteana@yahoogroups.com', 'text/plain; charset=\"US-ASCII\"', '7bit']\n",
      "\n",
      "\n",
      "Message Content: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<HTML><HEAD>\n",
      "<META content=\"text/html; charset=windows-1252\" http-equiv=Content-Type>\n",
      "<META content=\"MSHTML 5.00.2314.1000\" name=GENERATOR></HEAD>\n",
      "<BODY><!-- Inserted by Calypso -->\n",
      "<TABLE border=0 cellPadding=0 cellSpacing=2 id=_CalyPrintHeader_ rules=none \n",
      "style=\"COLOR: black; DISPLAY: none\" width=\"100%\">\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TD></TR>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TBODY></TABLE><!-- End Calypso --><!-- Inserted by Calypso --><FONT \n",
      "color=#000000 face=VERDANA,ARIAL,HELVETICA size=-2><BR></FONT></TD></TR></TABLE><!-- End Calypso --><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Save up to 70% on Life Insurance.</CENTER></FONT><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Why Spend More Than You Have To?\n",
      "<CENTER><FONT color=#ff0000 face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Life Quote Savings\n",
      "<CENTER>\n",
      "<P align=left></P>\n",
      "<P align=left></P></FONT></U></I></B><BR></FONT></U></B></U></I>\n",
      "<P></P>\n",
      "<CENTER>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=0 cellSpacing=0 width=650>\n",
      "  <TBODY></TBODY></TABLE>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=5 cellSpacing=0 width=650>\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=2 width=\"35%\"><B><FONT face=Verdana size=4>Ensuring your \n",
      "      family's financial security is very important. Life Quote Savings makes \n",
      "      buying life insurance simple and affordable. We Provide FREE Access to The \n",
      "      Very Best Companies and The Lowest Rates.</FONT></B></TD></TR>\n",
      "  <TR>\n",
      "    <TD align=middle vAlign=top width=\"18%\">\n",
      "      <TABLE borderColor=#111111 width=\"100%\">\n",
      "        <TBODY>\n",
      "        <TR>\n",
      "          <TD style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" width=\"100%\"><FONT \n",
      "            face=Verdana size=4><B>Life Quote Savings</B> is FAST, EASY and \n",
      "            SAVES you money! Let us help you get started with the best values in \n",
      "            the country on new coverage. You can SAVE hundreds or even thousands \n",
      "            of dollars by requesting a FREE quote from Lifequote Savings. Our \n",
      "            service will take you less than 5 minutes to complete. Shop and \n",
      "            compare. SAVE up to 70% on all types of Life insurance! \n",
      "</FONT></TD></TR>\n",
      "        <TR><BR><BR>\n",
      "          <TD height=50 style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" \n",
      "          width=\"100%\">\n",
      "            <P align=center><B><FONT face=Verdana size=5><A \n",
      "            href=\"http://website.e365.cc/savequote/\">Click Here For Your \n",
      "            Free Quote!</A></FONT></B></P></TD>\n",
      "          <P><FONT face=Verdana size=4><STRONG>\n",
      "          <CENTER>Protecting your family is the best investment you'll ever \n",
      "          make!<BR></B></TD></TR>\n",
      "        <TR><BR><BR></STRONG></FONT></TD></TR></TD></TR>\n",
      "        <TR></TR></TBODY></TABLE>\n",
      "      <P align=left><FONT face=\"Arial, Helvetica, sans-serif\" size=2></FONT></P>\n",
      "      <P></P>\n",
      "      <CENTER><BR><BR><BR>\n",
      "      <P></P>\n",
      "      <P align=left><BR></B><BR><BR><BR><BR></P>\n",
      "      <P align=center><BR></P>\n",
      "      <P align=left><BR></B><BR><BR></FONT>If you are in receipt of this email \n",
      "      in error and/or wish to be removed from our list, <A \n",
      "      href=\"mailto:coins@btamail.net.cn\">PLEASE CLICK HERE</A> AND TYPE REMOVE. If you \n",
      "      reside in any state which prohibits e-mail solicitations for insurance, \n",
      "      please disregard this \n",
      "      email.<BR></FONT><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR></FONT></P></CENTER></CENTER></TR></TBODY></TABLE></CENTER></CENTER></CENTER></CENTER></CENTER></BODY></HTML>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how emails are stored\n",
    "print('Header Field Names:', ham_emails[1].keys())\n",
    "print('\\n')\n",
    "print('Message Field Values:', ham_emails[1].values())\n",
    "print('\\n')\n",
    "print('Message Content:', spam_emails[1].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "In this section, the email structure will be extracted and content of the emails will be converted to plain text for the text analysis. This is executed through the functions: `get_email_structure()`, `structures_counter()`, `html_to_plain()` and `email_to_plain()` from the existing kernel$^4$. Two feature sets will be prepared for the modeling task. The `feature set 1` is based on **stopwords + n-gram + tf-idf** (which will be explained in the `feature set 1` section). The `feature set 2` is **most-frequent-word-count** based, developed from the existing kernel$^4$. \n",
    "\n",
    "Since the stopwords (the, is, of, a, to, from ..) eliminated from `feature set 1` are those words which occur most frequently from `feature set 2`, so the underlying ideas of these two feature sets are contradicting. Therefore they are divided into two feature sets and their model performances are reviewed separately.\n",
    "\n",
    "\n",
    "Both feature sets are generated by the **removal of punctuations**, **lower-casing**, and **word stemming**. \n",
    "\n",
    "Most of the machine learning algorithms understand numbers only. Vectorization is a process to convert a bag of words into an array of numbers, which is a representation of words. Consider a very long email with say 987,654 words in the training data. If each word is represented as a feature in a vector, then this vector will have a length of 987,654, however, this will introduce lots of zeros for a short email with say 123 words. In order to perform efficient operations, **Compressed Sparse Row (CSR)** represents three 1-D arrays for non-zero values, the extents of rows and the column indices so that features are encoded as **csr matrix** before modeling takes place.\n",
    "\n",
    "Let's start with getting the email structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    # payload is referred as content\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        # multipart: type of payload, a structured sequence of sub-messages each with own set of headers & payload\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "ham_structure = structures_counter(ham_emails)\n",
    "spam_structure = structures_counter(spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2453),\n",
       " ('multipart(text/plain, application/pgp-signature)', 72),\n",
       " ('multipart(text/plain, text/html)', 8),\n",
       " ('multipart(text/plain, text/plain)', 4),\n",
       " ('multipart(text/plain)', 3),\n",
       " ('multipart(text/plain, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, text/enriched)', 1),\n",
       " ('multipart(text/plain, application/ms-tnef, text/plain)', 1),\n",
       " ('multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)',\n",
       "  1),\n",
       " ('multipart(text/plain, video/mng)', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_structure.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 222),\n",
       " ('text/html', 181),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 19),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_structure.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\">\n",
      "<HTML><HEAD>\n",
      "<META content=\"text/html; charset=windows-1252\" http-equiv=Content-Type>\n",
      "<META content=\"MSHTML 5.00.2314.1000\" name=GENERATOR></HEAD>\n",
      "<BODY><!-- Inserted by Calypso -->\n",
      "<TABLE border=0 cellPadding=0 cellSpacing=2 id=_CalyPrintHeader_ rules=none \n",
      "style=\"COLOR: black; DISPLAY: none\" width=\"100%\">\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TD></TR>\n",
      "  <TR>\n",
      "    <TD colSpan=3>\n",
      "      <HR color=black noShade SIZE=1>\n",
      "    </TD></TR></TBODY></TABLE><!-- End Calypso --><!-- Inserted by Calypso --><FONT \n",
      "color=#000000 face=VERDANA,ARIAL,HELVETICA size=-2><BR></FONT></TD></TR></TABLE><!-- End Calypso --><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Save up to 70% on Life Insurance.</CENTER></FONT><FONT color=#ff0000 \n",
      "face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Why Spend More Than You Have To?\n",
      "<CENTER><FONT color=#ff0000 face=\"Copperplate Gothic Bold\" size=5 PTSIZE=\"10\">\n",
      "<CENTER>Life Quote Savings\n",
      "<CENTER>\n",
      "<P align=left></P>\n",
      "<P align=left></P></FONT></U></I></B><BR></FONT></U></B></U></I>\n",
      "<P></P>\n",
      "<CENTER>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=0 cellSpacing=0 width=650>\n",
      "  <TBODY></TBODY></TABLE>\n",
      "<TABLE border=0 borderColor=#111111 cellPadding=5 cellSpacing=0 width=650>\n",
      "  <TBODY>\n",
      "  <TR>\n",
      "    <TD colSpan=2 width=\"35%\"><B><FONT face=Verdana size=4>Ensuring your \n",
      "      family's financial security is very important. Life Quote Savings makes \n",
      "      buying life insurance simple and affordable. We Provide FREE Access to The \n",
      "      Very Best Companies and The Lowest Rates.</FONT></B></TD></TR>\n",
      "  <TR>\n",
      "    <TD align=middle vAlign=top width=\"18%\">\n",
      "      <TABLE borderColor=#111111 width=\"100%\">\n",
      "        <TBODY>\n",
      "        <TR>\n",
      "          <TD style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" width=\"100%\"><FONT \n",
      "            face=Verdana size=4><B>Life Quote Savings</B> is FAST, EASY and \n",
      "            SAVES you money! Let us help you get started with the best values in \n",
      "            the country on new coverage. You can SAVE hundreds or even thousands \n",
      "            of dollars by requesting a FREE quote from Lifequote Savings. Our \n",
      "            service will take you less than 5 minutes to complete. Shop and \n",
      "            compare. SAVE up to 70% on all types of Life insurance! \n",
      "</FONT></TD></TR>\n",
      "        <TR><BR><BR>\n",
      "          <TD height=50 style=\"PADDING-LEFT: 5px; PADDING-RIGHT: 5px\" \n",
      "          width=\"100%\">\n",
      "            <P align=center><B><FONT face=Verdana size=5><A \n",
      "            href=\"http://website.e365.cc/savequote/\">Click Here For Your \n",
      "            Free Quote!</A></FONT></B></P></TD>\n",
      "          <P><FONT face=Verdana size=4><STRONG>\n",
      "          <CENTER>Protecting your family is the best investment you'll ever \n",
      "          make!<BR></B></TD></TR>\n",
      "        <TR><BR><BR></STRONG></FONT></TD></TR></TD></TR>\n",
      "        <TR></TR></TBODY></TABLE>\n",
      "      <P align=left><FONT face=\"Arial, Helvetica, sans-serif\" size=2></FONT></P>\n",
      "      <P></P>\n",
      "      <CENTER><BR><BR><BR>\n",
      "      <P></P>\n",
      "      <P align=left><BR></B><BR><BR><BR><BR></P>\n",
      "      <P align=center><BR></P>\n",
      "      <P align=left><BR></B><BR><BR></FONT>If you are in receipt of this email \n",
      "      in error and/or wish to be removed from our list, <A \n",
      "      href=\"mailto:coins@btamail.net.cn\">PLEASE CLICK HERE</A> AND TYPE REMOVE. If you \n",
      "      reside in any state which prohibits e-mail solicitations for insurance, \n",
      "      please disregard this \n",
      "      email.<BR></FONT><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR><BR></FONT></P></CENTER></CENTER></TR></TBODY></TABLE></CENTER></CENTER></CENTER></CENTER></CENTER></BODY></HTML>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for email in spam_emails:\n",
    "    if get_email_structure(email) == 'text/html':\n",
    "        testEmail = email\n",
    "        break\n",
    "\n",
    "print(testEmail.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save up to 70% on Life Insurance.\n",
      "Why Spend More Than You Have To?Life Quote Savings\n",
      "Ensuring your \n",
      "      family's financial security is very important. Life Quote Savings makes \n",
      "      buying life insurance simple and affordable. We Provide FREE Access to The \n",
      "      Very Best Companies and The Lowest Rates.Life Quote Savings is FAST, EASY and \n",
      "            SAVES you money! Let us help you get started with the best values in \n",
      "            the country on new coverage. You can SAVE hundreds or even thousands \n",
      "            of dollars by requesting a FREE quote from Lifequote Savings. Our \n",
      "            service will take you less than 5 minutes to complete. Shop and \n",
      "            compare. SAVE up to 70% on all types of Life insurance! Click Here For Your \n",
      "            Free Quote!Protecting your family is the best investment you'll ever \n",
      "          make!\n",
      "If you are in receipt of this email \n",
      "      in error and/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE REMOVE. If you \n",
      "      reside in any state which prohibits e-mail solicitations for insurance, \n",
      "      please disregard this \n",
      "      email.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def html_to_plain(email):\n",
    "    try:\n",
    "        # parse data from html email content\n",
    "        soup = BeautifulSoup(email.get_content(), 'html.parser')\n",
    "        # return a clear form of text\n",
    "        return soup.text.replace('\\n\\n','')\n",
    "    except:\n",
    "        return \"empty\"\n",
    "\n",
    "print(html_to_plain(testEmail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ham email sample]: \n",
      "Joseph S. Barrera III wrote:\n",
      "\n",
      "> Chris Haun wrote:\n",
      ">\n",
      ">> A LifeGem is a certified, high quality diamond created from the \n",
      ">> carbon of your loved one as a memorial to their unique and wonderful \n",
      ">> life.\n",
      ">\n",
      ">\n",
      "> Why wait until you're dead? I'm sure there's enough carbon in\n",
      "> the fat from your typical liposuction job to make a decent diamond.\n",
      ">\n",
      "> - Joe\n",
      ">\n",
      "Oh, hell - what about excrement? I'd love to be able to say - No, the \n",
      "sun doesn't shine out of my ass, but there's the occasional diamond. ;-).\n",
      "\n",
      "Owen\n",
      "\n",
      "\n",
      "http://xent.com/mailman/listinfo/fork\n",
      "\n",
      "\n",
      "[spam email sample]: \n",
      "\n",
      "New Page 1\n",
      "VIAGRA\n",
      "WITHOUT\n",
      "A DOCTORS VISIT!!\n",
      "CLICK\n",
      "HERE\n",
      "*Other\n",
      "Top Medications also available!!\n",
      "*We\n",
      "have Doctors on call around the country to view\n",
      "your information and quickly approve your order.\n",
      "*Totally\n",
      "Discreet System allows you to order today and\n",
      "enjoy your medication tomorrow in most cases.\n",
      "*Finally\n",
      "you can try the wonder drug Viagra that\n",
      "has swept the World without the embarrassment of\n",
      "having to visit your Doctor and explain your condition!!\n",
      "TO\n",
      "ORDER CLICK HERE!\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "TO GET DELETED \n",
      "http://194.44.46.21/remove.php\n",
      " \n",
      " \n",
      "3606uLdz7-798Gxne6717WLiQ1-104VoKJ8349uvAE9-31l43\n"
     ]
    }
   ],
   "source": [
    "def email_to_plain(email):\n",
    "    struct = get_email_structure(email)\n",
    "    # walk(): iterate over all the parts and subparts of a message object tree\n",
    "    for part in email.walk():\n",
    "        partContentType = part.get_content_type()\n",
    "        if partContentType not in ['text/plain','text/html']:\n",
    "            continue\n",
    "        try:\n",
    "            partContent = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            partContent = str(part.get_payload())\n",
    "        if partContentType == 'text/plain':\n",
    "            return partContent\n",
    "        else:\n",
    "            return html_to_plain(part)\n",
    "\n",
    "print(\"[ham email sample]: \")        \n",
    "print(email_to_plain(ham_emails[42]))\n",
    "print(\"[spam email sample]: \")\n",
    "print(email_to_plain(spam_emails[42]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `feature set 1`: \"stopwords + n-grams + tf-idf\" \n",
    "The `feature set 1` is created by exploring the text structure to exploit the contextual features. In particular, the following operations **stopwords**, **n-grams** and **tf-idf** will be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stopwords\n",
    "Stopwords are words that appear frequently in text but have little lexical content. Their presence creates wrong signal for prediction. Let us explore the common stopwords from the Natural Language Toolkit `nltk` package, a popular library for natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming\n",
    "\"Laptop\" and \"Laptops\" are just two forms of the same dictionary lemma. In linguistic morphology and information retrieval, stemming is the process of reducing inflected or derived words into their word stem, base or root form.\n",
    "\n",
    "In the `nltk` library a commonly used stemmer is PorterStemmer. It is originated from 80's with the aim to reduce words to its common form. Let us start with this basic stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Living => live\n",
      "Live => live\n",
      "Lives => live\n",
      "Lived => live\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "for word in (\"Living\", \"Live\", \"Lives\", \"Lived\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalisation\n",
    "To ensure that certain form of expressions are in the same notation in the training data, regular expressions are used to detect word patterns. These include extracting email addresses, Web URL, amount of money starting with curreny symbol followed by numbers, phone numbers, numbers, punctuation marks, and whitespaces and grouping them to the same terms. This also includes returning all characters to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us include all the above considerations in a function `transformation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(X):\n",
    "        X_to_words = []\n",
    "        for email in X:\n",
    "            text = email_to_plain(email)\n",
    "            \n",
    "            try:\n",
    "                text = text.replace(\"\\n\", \" \")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            text = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', str(text))\n",
    "            text = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', text)\n",
    "            text = re.sub(r'£|€|\\$', 'currencysymb', text)\n",
    "            text = re.sub(r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b','phonenumbr', text)\n",
    "            text = re.sub(r'\\d+(\\.\\d+)?', 'numbr', text)\n",
    "            text = re.sub(r'[^\\w\\d\\s]', ' ', text)\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            text = re.sub(r'^\\s+|\\s+?$', '', text.lower())\n",
    "            \n",
    "            processed_text = ' '.join(stemmer.stem(term) for term in text.split() if term not in set(stop_words))\n",
    "            \n",
    "            X_to_words.append(processed_text)\n",
    "\n",
    "        return X_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[email before transformation()]: \n",
      "1) Fight The Risk of Cancer!\n",
      "http://www.adclick.ws/p.cfm?o=315&s=pk007\n",
      "\n",
      "2) Slim Down - Guaranteed to lose 10-12 lbs in 30 days\n",
      "http://www.adclick.ws/p.cfm?o=249&s=pk007\n",
      "\n",
      "3) Get the Child Support You Deserve - Free Legal Advice\n",
      "http://www.adclick.ws/p.cfm?o=245&s=pk002\n",
      "\n",
      "4) Join the Web's Fastest Growing Singles Community\n",
      "http://www.adclick.ws/p.cfm?o=259&s=pk007\n",
      "\n",
      "5) Start Your Private Photo Album Online!\n",
      "http://www.adclick.ws/p.cfm?o=283&s=pk007\n",
      "\n",
      "Have a Wonderful Day,\n",
      "Offer Manager\n",
      "PrizeMama\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you wish to leave this list please use the link below.\n",
      "http://www.qves.com/trim/?ilug@linux.ie%7C17%7C114258\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "\n",
      "[email after transformation()]: \n",
      "numbr fight risk cancer httpaddr numbr slim guarante lose numbr numbr lb numbr day httpaddr numbr get child support deserv free legal advic httpaddr numbr join web fastest grow singl commun httpaddr numbr start privat photo album onlin httpaddr wonder day offer manag prizemama wish leav list pleas use link httpaddr irish linux user group emailaddr httpaddr un subscript inform list maintain emailaddr\n"
     ]
    }
   ],
   "source": [
    "# email before and after transformation()\n",
    "print(\"[email before transformation()]: \")\n",
    "print(email_to_plain(spam_emails[2]))\n",
    "print(\"[email after transformation()]: \")\n",
    "print(transformation(spam_emails)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming email groups...\n",
      "Transformation done!\n",
      "CPU times: user 35.6 s, sys: 204 ms, total: 35.8 s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Transforming email groups...\")\n",
    "transformed_ham_emails = transformation(ham_emails)\n",
    "transformed_spam_emails = transformation(spam_emails)\n",
    "transformed_ham_spam_emails = transformation(ham_emails + spam_emails)\n",
    "print(\"Transformation done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization\n",
    "Individual terms can be tokenized and a bag of words model is generated. Consequently, every sequence of *n* terms called n-grams preserving word order can potentially capture more information than a bag of words.\n",
    "A n-gram model models sequences, predicts $x_i$ based on previous words $x_{i-1}$, ... , $x_{i-(n-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term Frequency Inverse Document Frequency (tf-idf)\n",
    "After selecting the preferred n-grams, the frequency of each n-gram can be computed by term frequency *tf* and inverse document frequency *idf*.\n",
    "\n",
    "*tf* computes the frequency of each term, however, some n-grams appear more offen in multiple emails, while some appear less but relatively more in spam emails. To address this, idf calculates the logarithmically inverse fraction of training records that contain the term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer()` from the `scikit-learn` library provides the complete framework to select n-grams, compute their tf-idf statistic and transform the data in the compressed sparse row matrix format, which is ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams are chosen in ngram_range selection\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 49.9 ms, total: 1.27 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_ngrams = vectorizer.fit_transform(transformed_ham_spam_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3052x194096 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 365990 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `feature set 2`: \"most-frequent-word-count\" \n",
    "The idea of generating `feature set 2` is different from `feature set 1`, which is based on counting the most frequently occuring words from the email content.\n",
    "\n",
    "The classes `email_to_words()` and `word_count_to_vector()` follow the existing kernel$^4$. `email_to_words()` serves to transform the plain text to word frequencies through punctuation marks removal, lower case conversion and stemming. `word_count_to_vector()` converts word frequencies to a compressed sparse row matrix format for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class email_to_words(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "    def __init__(self, \n",
    "                 lowercaseConversion = True, \n",
    "                 punctuationRemoval = True, \n",
    "                 stemming = True):\n",
    "        self.lowercaseConversion = lowercaseConversion\n",
    "        self.punctuationRemoval = punctuationRemoval\n",
    "        self.stemming = stemming\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X_to_words = []\n",
    "        for email in X:\n",
    "            text = email_to_plain(email)\n",
    "            \n",
    "            if text is None:\n",
    "                text = 'empty'\n",
    "            \n",
    "            if self.lowercaseConversion:\n",
    "                text = text.lower()\n",
    "                        \n",
    "            if self.punctuationRemoval:\n",
    "                text = text.replace('.','')\n",
    "                text = text.replace(',','')\n",
    "                text = text.replace('!','')\n",
    "                text = text.replace('?','')\n",
    "                \n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming:\n",
    "                stemmed_word_count = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = self.stemmer.stem(word)\n",
    "                    stemmed_word_count[stemmed_word] += count\n",
    "                word_counts = stemmed_word_count\n",
    "            X_to_words.append(word_counts)\n",
    "            \n",
    "        return np.array(X_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[email before email_to_plain()]: \n",
      "    Date:        Wed, 21 Aug 2002 10:54:46 -0500\n",
      "    From:        Chris Garrigues <cwg-dated-1030377287.06fa6d@DeepEddy.Com>\n",
      "    Message-ID:  <1029945287.4797.TMDA@deepeddy.vircio.com>\n",
      "\n",
      "\n",
      "  | I can't reproduce this error.\n",
      "\n",
      "For me it is very repeatable... (like every time, without fail).\n",
      "\n",
      "This is the debug log of the pick happening ...\n",
      "\n",
      "18:19:03 Pick_It {exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace} {4852-4852 -sequence mercury}\n",
      "18:19:03 exec pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace 4852-4852 -sequence mercury\n",
      "18:19:04 Ftoc_PickMsgs {{1 hit}}\n",
      "18:19:04 Marking 1 hits\n",
      "18:19:04 tkerror: syntax error in expression \"int ...\n",
      "\n",
      "Note, if I run the pick command by hand ...\n",
      "\n",
      "delta$ pick +inbox -list -lbrace -lbrace -subject ftp -rbrace -rbrace  4852-4852 -sequence mercury\n",
      "1 hit\n",
      "\n",
      "That's where the \"1 hit\" comes from (obviously).  The version of nmh I'm\n",
      "using is ...\n",
      "\n",
      "delta$ pick -version\n",
      "pick -- nmh-1.0.4 [compiled on fuchsia.cs.mu.OZ.AU at Sun Mar 17 14:55:56 ICT 2002]\n",
      "\n",
      "And the relevant part of my .mh_profile ...\n",
      "\n",
      "delta$ mhparam pick\n",
      "-seq sel -list\n",
      "\n",
      "\n",
      "Since the pick command works, the sequence (actually, both of them, the\n",
      "one that's explicit on the command line, from the search popup, and the\n",
      "one that comes from .mh_profile) do get created.\n",
      "\n",
      "kre\n",
      "\n",
      "ps: this is still using the version of the code form a day ago, I haven't\n",
      "been able to reach the cvs repository today (local routing issue I think).\n",
      "\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Exmh-workers mailing list\n",
      "Exmh-workers@redhat.com\n",
      "https://listman.redhat.com/mailman/listinfo/exmh-workers\n",
      "\n",
      "\n",
      "[email after email_to_plain(): \n",
      "[Counter({'the': 15, 'pick': 9, '-lbrace': 6, 'of': 5, '-rbrace': 5, 'i': 4, 'is': 4, '-list': 4, 'thi': 3, '+inbox': 3, '-subject': 3, 'ftp': 3, '-sequenc': 3, '18:19:04': 3, 'command': 3, 'delta$': 3, 'from': 3, 'error': 2, '18:19:03': 2, '4852-4852': 2, 'mercuri': 2, '1': 2, 'hit': 2, \"that'\": 2, 'come': 2, 'version': 2, 'use': 2, 'on': 2, 'and': 2, 'one': 2, 'date:': 1, 'wed': 1, '21': 1, 'aug': 1, '2002': 1, '10:54:46': 1, '-0500': 1, 'from:': 1, 'chri': 1, 'garrigu': 1, '<cwg-dated-103037728706fa6d@deepeddycom>': 1, 'message-id:': 1, '<10299452874797tmda@deepeddyvirciocom>': 1, '|': 1, \"can't\": 1, 'reproduc': 1, 'for': 1, 'me': 1, 'it': 1, 'veri': 1, 'repeat': 1, '(like': 1, 'everi': 1, 'time': 1, 'without': 1, 'fail)': 1, 'debug': 1, 'log': 1, 'happen': 1, 'pick_it': 1, '{exec': 1, '-rbrace}': 1, '{4852-4852': 1, 'mercury}': 1, 'exec': 1, 'ftoc_pickmsg': 1, '{{1': 1, 'hit}}': 1, 'mark': 1, 'tkerror:': 1, 'syntax': 1, 'in': 1, 'express': 1, '\"int': 1, 'note': 1, 'if': 1, 'run': 1, 'by': 1, 'hand': 1, 'where': 1, '\"1': 1, 'hit\"': 1, '(obviously)': 1, 'nmh': 1, \"i'm\": 1, '-version': 1, '--': 1, 'nmh-104': 1, '[compil': 1, 'fuchsiacsmuozau': 1, 'at': 1, 'sun': 1, 'mar': 1, '17': 1, '14:55:56': 1, 'ict': 1, '2002]': 1, 'relev': 1, 'part': 1, 'my': 1, 'mh_profil': 1, 'mhparam': 1, '-seq': 1, 'sel': 1, 'sinc': 1, 'work': 1, 'sequenc': 1, '(actual': 1, 'both': 1, 'them': 1, 'explicit': 1, 'line': 1, 'search': 1, 'popup': 1, 'that': 1, 'mh_profile)': 1, 'do': 1, 'get': 1, 'creat': 1, 'kre': 1, 'ps:': 1, 'still': 1, 'code': 1, 'form': 1, 'a': 1, 'day': 1, 'ago': 1, \"haven't\": 1, 'been': 1, 'abl': 1, 'to': 1, 'reach': 1, 'cv': 1, 'repositori': 1, 'today': 1, '(local': 1, 'rout': 1, 'issu': 1, 'think)': 1, '_______________________________________________': 1, 'exmh-work': 1, 'mail': 1, 'list': 1, 'exmh-workers@redhatcom': 1, 'https://listmanredhatcom/mailman/listinfo/exmh-work': 1})]\n"
     ]
    }
   ],
   "source": [
    "# email before and after email_to_plain()\n",
    "print(\"[email before email_to_plain()]: \")\n",
    "print(email_to_plain(ham_emails[0]))\n",
    "print(\"[email after email_to_plain(): \")\n",
    "print(email_to_words().transform(ham_emails[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the emails, the above is a list of words with their frequencies for each email. The next step is to choose which words will be included for the spam detection and which words are excluded.\n",
    "\n",
    "Since rarely occuring words only exist in a few emails, they may cause the model to overfit (which will be explained in the modeling section) the training data. Hence, only the **most frequently occuring words** will be considered to create a vocabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_count_to_vector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size = 1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        total_word_count = Counter()\n",
    "        for word_count in X: # word_count: whole dictionary in Counter()\n",
    "            for word, count in word_count.items():\n",
    "                total_word_count[word] += count\n",
    "        self.most_common = total_word_count.most_common()[:self.vocabulary_size] # list the most common words and their frequencies\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(self.most_common)} # list the rank of the most common words\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape = (len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[email before word_count_to_vector()]: \n",
      "[Counter({'the': 15, 'pick': 9, '-lbrace': 6, 'of': 5, '-rbrace': 5, 'i': 4, 'is': 4, '-list': 4, 'thi': 3, '+inbox': 3, '-subject': 3, 'ftp': 3, '-sequenc': 3, '18:19:04': 3, 'command': 3, 'delta$': 3, 'from': 3, 'error': 2, '18:19:03': 2, '4852-4852': 2, 'mercuri': 2, '1': 2, 'hit': 2, \"that'\": 2, 'come': 2, 'version': 2, 'use': 2, 'on': 2, 'and': 2, 'one': 2, 'date:': 1, 'wed': 1, '21': 1, 'aug': 1, '2002': 1, '10:54:46': 1, '-0500': 1, 'from:': 1, 'chri': 1, 'garrigu': 1, '<cwg-dated-103037728706fa6d@deepeddycom>': 1, 'message-id:': 1, '<10299452874797tmda@deepeddyvirciocom>': 1, '|': 1, \"can't\": 1, 'reproduc': 1, 'for': 1, 'me': 1, 'it': 1, 'veri': 1, 'repeat': 1, '(like': 1, 'everi': 1, 'time': 1, 'without': 1, 'fail)': 1, 'debug': 1, 'log': 1, 'happen': 1, 'pick_it': 1, '{exec': 1, '-rbrace}': 1, '{4852-4852': 1, 'mercury}': 1, 'exec': 1, 'ftoc_pickmsg': 1, '{{1': 1, 'hit}}': 1, 'mark': 1, 'tkerror:': 1, 'syntax': 1, 'in': 1, 'express': 1, '\"int': 1, 'note': 1, 'if': 1, 'run': 1, 'by': 1, 'hand': 1, 'where': 1, '\"1': 1, 'hit\"': 1, '(obviously)': 1, 'nmh': 1, \"i'm\": 1, '-version': 1, '--': 1, 'nmh-104': 1, '[compil': 1, 'fuchsiacsmuozau': 1, 'at': 1, 'sun': 1, 'mar': 1, '17': 1, '14:55:56': 1, 'ict': 1, '2002]': 1, 'relev': 1, 'part': 1, 'my': 1, 'mh_profil': 1, 'mhparam': 1, '-seq': 1, 'sel': 1, 'sinc': 1, 'work': 1, 'sequenc': 1, '(actual': 1, 'both': 1, 'them': 1, 'explicit': 1, 'line': 1, 'search': 1, 'popup': 1, 'that': 1, 'mh_profile)': 1, 'do': 1, 'get': 1, 'creat': 1, 'kre': 1, 'ps:': 1, 'still': 1, 'code': 1, 'form': 1, 'a': 1, 'day': 1, 'ago': 1, \"haven't\": 1, 'been': 1, 'abl': 1, 'to': 1, 'reach': 1, 'cv': 1, 'repositori': 1, 'today': 1, '(local': 1, 'rout': 1, 'issu': 1, 'think)': 1, '_______________________________________________': 1, 'exmh-work': 1, 'mail': 1, 'list': 1, 'exmh-workers@redhatcom': 1, 'https://listmanredhatcom/mailman/listinfo/exmh-work': 1})]\n",
      "[email after word_count_to_vector()]: \n",
      "[[ 0 15  9 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"[email before word_count_to_vector()]: \")\n",
    "print(email_to_words().transform(ham_emails[:1]))\n",
    "print(\"[email after word_count_to_vector()]: \")\n",
    "print(word_count_to_vector(vocabulary_size = 1000).fit_transform(email_to_words().transform(ham_emails[:1])).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pipeline = Pipeline([\n",
    "    (\"Email to Words\", email_to_words()),\n",
    "    (\"Wordcount to Vector\", word_count_to_vector()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling\n",
    "A lot of classification algorithms are available. According to the readme documentation of SpamAssassin$^6$, our 2500 non-spam messages belong to easy_ham and they should be easily differentiated from spam. Therefore, instead of using sophisicated and hybrid models, I rely on relatively simple classification algorithms to solve this problem.\n",
    "\n",
    "In this section a modeling pipeline using different classifiers and feature sets will be run. Also, identification of model underfitting and overfitting will be discussed. To prevent underfitting and overfitting, the modeling results will be evaluated first through the cross-validation score, and then evaluated by evaluation metrics of classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy ####\n",
    "The first group of classifier is the Dummy Classifier, which provides measures of \"baseline\" performances. \n",
    "There are three strategy options for the Dummy Classifier. The default strategy is **stratified**. In our case Dummy Classifier will predict that there is a 16% (spam to all ratio) probability that each new record in the test set possesses the target property (spam). \n",
    "\n",
    "Another strategy is **most frequent**. In our case since ham emails dominate all emails (84% among all), Dummy Classifier will guess that every new record in the test set gives the majority guess, that is 0. There will be **no false positives (FP)**. This leads to perfect precision.\n",
    "\n",
    "The third strategy is **constant**. Let us set the constant value to be 1 from the minority group as the counter strategy of most frequent. Since all cases are classified as spams, there will be **no false negatives (FN)**. This leads to perfect recall rate. \n",
    "\n",
    "The goal of these three classifiers is to provide the baseline performances based on the given target distribution only. In our modeling task we aim to come up with a model which at least beats the baseline model. Otherwise, we do not have to go into the email at all and extract features for modeling.\n",
    "\n",
    "#### Logistic Regression (LR) ####\n",
    "Logistic Regression, aka maximum-entropy classification identifies a set of parameters by maximizing the performance of the classifer, the total likelihood of the training corpus. In other words, it maximizes the likelihood function or entropy to identify parameters which fits the data through the sigmoid transformation. It is a simple method to classify data.\n",
    "\n",
    "\n",
    "#### Multinomial NB #### \n",
    "Naïve Bayes, aka Bayesian Filter, has been known to perform well in the domain of spam detection. It is easy to implement, and its computational complexity and accuracy is comparable to other sophisticated machine learning algorithms. Its simplicity is based on the assumption of conditional independence between each pair of features given the value of the class. \n",
    "\n",
    "In n-gram probability model, the probability of a word conditional on some number of previous words follows a multinomial distribution. Counting words is modeling probability of word occurence. Therefore, multinomial distribution is best suitable as the underlying distribution assumption of Naïve Bayes.\n",
    "\n",
    "\n",
    "#### Support Vector Machine (SVM) - Linear Kernel ####\n",
    "Support vector machines (SVMs) finds hyperplanes for separation. It has been known to perform well with small datasets. It is not overly influenced by noisy data and not very prone to overfitting. A linear kernel is chosen due to the large amount of features. Using nonlinear kernel would be computational expensive.\n",
    "\n",
    "\n",
    "#### Random Forest (RF) ####\n",
    "Random Forest is the ensemble learning technique which aggregates the results of multiple trees. These trees are uncorrelated to each other, therefore random forest is a stronger classifier than a single decision tree. It has been used for phishing email classification before NLP gains its popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Underfitting and Overfitting ####\n",
    "If a model is not sufficient to fit the training data, then the model suffers from underfitting. If a model fits too specific to the training data, it learns the noise, overfits the training data and such a model cannot generalise to unseen data. Overfit can occur due to too many features in a small training dataset or the use of complicated models. \n",
    "A perfect (or best) model should be the one which reduces underfitting or overfitting. There are three practices for identification. They are datasets splitting, cross-validation and bootstrap.\n",
    "\n",
    "One of the most common practices is to **split datasets** into training set, validation set and test set, where training set is used to train on the data and generate the model, validation is used to evaluate the model generated and select the best model, while test set serves as final assertion. This method is subject to the size of the training set. The modeling performance may be affected if the training set is too small.\n",
    "\n",
    "Another evaluation to quantify underfitting / overfitting is **cross-validation** which generalises the model performance using the same training data. The idea is to split the training dataset into subsets and each subset will be used as hold-out \"test\" set to validate the trained model. Averaging results will provide final model performance. Estimates from cross-validation is still biased, but bias reduced with the number of folds. \n",
    "\n",
    "**Bootstrap** is the advanced version of cross-validation. A simple version is leave-one-out (LOO), in which every single record will be left out in the training set and can be treated as a \"test\" record to validate the trained model. The model estimate of bootstrap is least biased but it is most computational expensive.\n",
    "\n",
    "To take a balance among all methodologies, 10-fold cross-validation will be conducted with the reported mean and the standard deviation (std) of cv-score. CV-score in cross-validation is also the accuracy score in the classification task. Mean of the cv-score can be compared with the accuracy of the model, while standard deviation checks how dispersed the estimates are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics ####\n",
    "\"Predicted\" and \"identified\" have the same meaning in the following definitions.\n",
    "- True Positives (TP) are the number of predicted spams which are correctly identified.\n",
    "- True Negatives (TN) are the number of predicted hams which are correctly identified.\n",
    "- False Positives (FP) are the number of predicted spams which are incorrectly identified.\n",
    "- False Negatives (FN) are the number of predicted hams which are incorrectly identified.\n",
    "- **Accuracy** is the simplest metric to evaluate a classifier which is calculated as $\\frac{TP + TN}{TP + TN + FP + FN}$. It measures the percentage of inputs in the test set that the classifier correctly labeled, in our case, the ratio of correctly identified hams and spams out of all emails.\n",
    "- **Precision** indicates the fraction of relevant items among all the retrieved items$^7$, which is computed as $\\frac{TP}{TP + FP}$. This is the correctly identified spams out of all the spams identified.\n",
    "- **Recall** indicates the fraction of the relevant items that are actually retrieved$^7$, which is computed as $\\frac{TP}{TP + FN}$. This is the correctly identified spams out of all actual spams.\n",
    "- **F1**, aka F-Measure (or F-Score), which combines the precision and recall to give a single score, is defined as the harmonic mean of the precision and recall: $\\frac{2 × Precision × Recall}{Precision + Recall}$. In other words, it takes both false positives and false negatives into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting `random_state = 42` to a specific number is used to initialize a pseudorandom number generator, such that the modeling results are reproducible. \n",
    "\n",
    "`stratify = y` in the `train_test_split()` parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dummy_Stratified():\n",
    "    return DummyClassifier(random_state = 42, strategy = \"stratified\")\n",
    "\n",
    "def Dummy_Frequent():\n",
    "    return DummyClassifier(random_state = 42, strategy = \"most_frequent\")\n",
    "\n",
    "def Dummy_Constant():\n",
    "    return DummyClassifier(constant = 1, random_state = 42, strategy = \"constant\")\n",
    "\n",
    "def NB_Multinomial():\n",
    "    return MultinomialNB()\n",
    "   \n",
    "def LR():\n",
    "    return LogisticRegression(solver = \"liblinear\", random_state = 42)\n",
    "    \n",
    "def RF():\n",
    "    return RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "def SVM():\n",
    "    return svm.LinearSVC(random_state = 42)\n",
    "\n",
    "switcher = {\n",
    "    0: Dummy_Stratified,\n",
    "    1: Dummy_Frequent,\n",
    "    2: Dummy_Constant,\n",
    "    3: NB_Multinomial,\n",
    "    4: LR,\n",
    "    5: RF,\n",
    "    6: SVM\n",
    "}\n",
    "\n",
    "def model_choice(i):\n",
    "    func = switcher.get(i, lambda: 'Invalid')\n",
    "    return func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(model_name, feature):\n",
    "    result = {}\n",
    "    \n",
    "    X = np.array(ham_emails + spam_emails) if feature == \"word-count\" else X_ngrams\n",
    "    y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 42, \n",
    "                                                        stratify = y)\n",
    "    \n",
    "    # get modeling instance through model choice by model_name\n",
    "    clf = model_choice({v:k for k, v in switcher.items()}[model_name])\n",
    "    \n",
    "    # fit_transform() joins word-count-fit and transform x as initial step\n",
    "    X_augmented_train = email_pipeline.fit_transform(X_train) if feature == \"word-count\" else X_train\n",
    "\n",
    "    # run CV to guarantee consistent performance\n",
    "    # dense array is required for NB\n",
    "    cv_score = cross_val_score(clf, X_augmented_train, y_train, cv = 10) if model_name.__name__[0:2] != \"NB\" else cross_val_score(clf, X_augmented_train.toarray(), y_train, cv = 10)     \n",
    "    cv_score_mean = cv_score.mean()\n",
    "    cv_score_sd = cv_score.std()\n",
    "    \n",
    "    # transform() apply tranformation on the fitted internal object   \n",
    "    X_augmented_test = email_pipeline.transform(X_test) if feature == \"word-count\" else X_test\n",
    "    clf.fit(X_augmented_train, y_train) if model_name.__name__[0:2] != \"NB\" else clf.fit(X_augmented_train.toarray(), y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_augmented_test) if model_name.__name__[0:2] != \"NB\" else clf.predict(X_augmented_test.toarray())\n",
    "\n",
    "    ac = accuracy_score(y_test, y_pred)    \n",
    "    pc = precision_score(y_test, y_pred)\n",
    "    rc = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    result = {'feature': feature,\n",
    "              'model_name': model_name, \n",
    "              'clf': clf,\n",
    "              'cv_score': cv_score,\n",
    "              'accuracy': ac,\n",
    "              'precision': pc,\n",
    "              'recall': rc,\n",
    "              'F1': f1,\n",
    "              'y_test': y_test,\n",
    "              'y_pred': y_pred\n",
    "             }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_result(model_seq, feature):\n",
    "    data = []\n",
    "    for j in model_seq:\n",
    "        m = model(j, feature)\n",
    "        data.append([m['feature'],\n",
    "                    m['model_name'].__name__,\n",
    "                    float(\"{0:.4f}\".format(m['cv_score'].mean())), \n",
    "                    float(\"{0:.4f}\".format(m['cv_score'].std())), \n",
    "                    float(\"{0:.4f}\".format(m['accuracy'])), \n",
    "                    float(\"{0:.4f}\".format(m['precision'])), \n",
    "                    float(\"{0:.4f}\".format(m['recall'])), \n",
    "                    float(\"{0:.4f}\".format(m['F1']))])\n",
    "        df = pd.DataFrame(data)\n",
    "        df.columns = ['feature', 'model_name', 'cv_score_mean', 'cv_score_std', 'accuracy', 'precision', 'recall', 'F1']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results\n",
    "In this section I am going to discuss the results based on the above modeling pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ms, sys: 9.2 ms, total: 122 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_result_dummy = model_result({Dummy_Stratified: 'Dummy_Stratified', Dummy_Frequent: 'Dummy_Frequent', Dummy_Constant: 'Dummy_Constant'}, \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 469 ms, total: 56.4 s\n",
      "Wall time: 56.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_result_wc = model_result({NB_Multinomial: 'NB_Multinomial', LR: 'LR', RF: 'RF', SVM: 'SVM'}, \"word-count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 1min 2s, total: 2min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_result_stopword_ngram_tdidf = model_result({NB_Multinomial: 'NB_Multinomial', LR: 'LR', RF: 'RF', SVM: 'SVM'}, \"stopword + n-gram + td-idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_score_mean</th>\n",
       "      <th>cv_score_std</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>Dummy_Stratified</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.7381</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>Dummy_Frequent</td>\n",
       "      <td>0.8357</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.8363</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>Dummy_Constant</td>\n",
       "      <td>0.1643</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.2813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature        model_name  cv_score_mean  cv_score_std  accuracy  precision  \\\n",
       "0       -  Dummy_Stratified         0.7432        0.0177    0.7381     0.2222   \n",
       "1       -    Dummy_Frequent         0.8357        0.0010    0.8363     0.0000   \n",
       "2       -    Dummy_Constant         0.1643        0.0010    0.1637     0.1637   \n",
       "\n",
       "   recall      F1  \n",
       "0    0.24  0.2308  \n",
       "1    0.00  0.0000  \n",
       "2    1.00  0.2813  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_score_mean</th>\n",
       "      <th>cv_score_std</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>word-count</td>\n",
       "      <td>NB_Multinomial</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.9787</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.9333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>word-count</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.9885</td>\n",
       "      <td>0.9895</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.9641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>word-count</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.9223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>word-count</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>0.9505</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.9552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature      model_name  cv_score_mean  cv_score_std  accuracy  \\\n",
       "0  word-count  NB_Multinomial         0.9803        0.0073    0.9787   \n",
       "1  word-count              LR         0.9865        0.0080    0.9885   \n",
       "2  word-count              RF         0.9767        0.0082    0.9755   \n",
       "3  word-count             SVM         0.9791        0.0097    0.9853   \n",
       "\n",
       "   precision  recall      F1  \n",
       "0     0.9579    0.91  0.9333  \n",
       "1     0.9895    0.94  0.9641  \n",
       "2     0.9570    0.89  0.9223  \n",
       "3     0.9505    0.96  0.9552  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>model_name</th>\n",
       "      <th>cv_score_mean</th>\n",
       "      <th>cv_score_std</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>stopword + n-gram + td-idf</td>\n",
       "      <td>NB_Multinomial</td>\n",
       "      <td>0.9222</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.7730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>stopword + n-gram + td-idf</td>\n",
       "      <td>LR</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.4962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>stopword + n-gram + td-idf</td>\n",
       "      <td>RF</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.8887</td>\n",
       "      <td>0.6159</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.7143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>stopword + n-gram + td-idf</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature      model_name  cv_score_mean  cv_score_std  \\\n",
       "0  stopword + n-gram + td-idf  NB_Multinomial         0.9222        0.0162   \n",
       "1  stopword + n-gram + td-idf              LR         0.8849        0.0094   \n",
       "2  stopword + n-gram + td-idf              RF         0.9062        0.0171   \n",
       "3  stopword + n-gram + td-idf             SVM         0.9509        0.0153   \n",
       "\n",
       "   accuracy  precision  recall      F1  \n",
       "0    0.9394     1.0000    0.63  0.7730  \n",
       "1    0.8903     1.0000    0.33  0.4962  \n",
       "2    0.8887     0.6159    0.85  0.7143  \n",
       "3    0.9591     1.0000    0.75  0.8571  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result_stopword_ngram_tdidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model: Dummy\n",
    "The Dummy models serve the base models which rely on the distribution of the target variable (spam or ham) only. They are independent of our selected features. \n",
    "\n",
    "The Stratified Dummy predicts 84% of emails ham and 16% of emails spam gives the accuracy of 74%. \n",
    "\n",
    "The Most Frequent Dummy predicts all emails ham which predicts all hams perfect but has no predictive power at all in spam predictions. True Positives are all zeros, which make precision, recall and F1 all undefined, even 0 is the default output for undefined values in the `sklearn.metrics`. Its high accuracy is due to the high ratio of hams out of all emails.\n",
    "\n",
    "The Constant Dummy 1 predicts every email as spam and makes it a perfect spam detector but has no predictive power at all in predicting hams. \n",
    "\n",
    "In this modeling task, we hope to build a classifier with model performance at least better than the base model performance, which is 0.8357 in accuracy.\n",
    "\n",
    "##### `Feature Set 1`: stopwords + n-gram + td-idf\n",
    "Among all models using the `feature 1` **stopwords + n-gram + td-idf**, **Support Vector Machine** with **linear kernel** gives the best model performance in terms of **accuracy and F1**. \n",
    "\n",
    "##### `Feature Set 2`: most-frequent-word-count\n",
    "Among all models using the `feature 2` **most-frequent-word-count**, **Logistic Regression** followed by multinomial Naïve Bayes gives the best model performance in terms of **accuracy, precision, and F1**, though multinomial Naïve Bayes gives a smaller standard deviation of cross-validation score than logistic regression (0.0073 vs 0.008), suggesting that logistic regression may suffer a bit from overfitting. Mean of cross-validation scores are quite close to the accuracy, suggesting that all models are not suffering from underfitting.\n",
    "\n",
    "##### Feature Comparison: stopwords + n-gram + td-idf vs most-frequent-word-count\n",
    "All the models based on `feature set 2` **most-frequent-word-count** have a higher accuracy and F1 score than those based on the `feature set 1` **stopwords + n-gram + td-idf**. In the `feature set 1` **stopwords + n-gram + td-idf**, except random forest, precision from the other models has the perfect precision rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusions \n",
    "Which is the best classifier? It depends on the application of spam email detector. \n",
    "\n",
    "If the use case is to introduce a beta version of an email spam detector like **no-spam in inbox**, then false negatives should be minimized and precision will be the most important evaluation metric. In this case, the model: **Support Vector Machine** with **linear kernel** from the `feature set 1` **stopwords + n-gram + tfidf** serves this suppose.\n",
    "\n",
    "If the use case is to introduce an email spam detector to **reduce bad user experience** in searching for important emails from junk mailbox and filtering spams from indox, then accuracy and F1 give a balance of reducing false postivies and false negatives. In this case **Logistic Regression** with `feature set 2` **most-frequent-word-count** gives a better user experience in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improvements\n",
    "This is the very initial version of the Spam Email Detection. Based on the above discussions, the following points can be taken further into account.\n",
    "\n",
    "- Features\n",
    "    - Some spam emails are dominated by capital letters. In our text preprocessing all letters are converted into  small letters which uncover potential signals in spam prediction. The ratio of capital letters in text can be used as feature.\n",
    "    - Currently features are extracted based on simple rules and regular expressions. More sophisticated features can be explored$^8$.\n",
    "    - Compare with `feature set 2` **most-frequent-word-count**, the current `feature set 1` **stopwords + n-gram + tf-idf** is not performing so well. One suggestion is to refine words in the stopwords list. Another suggestion is to try other stemmers, because the current PorkerStemmer is a relatively gentle stemming algorithm and its development is paused.\n",
    "    \n",
    "- Modeling\n",
    "    - Grid search CV can be used after model selection to optimise hyperparameters in model tuning.\n",
    "    \n",
    "- Evaluation\n",
    "    - To compare models with similar performances, confusion matrix can be used to explore instances of false positives and false negatives. \n",
    "    \n",
    "- Code Refactoring\n",
    "    - Generation of `feature set 1` and `feature set 2` is written based on different data structures. If the developed text preprocessing pipeline is useful in reviewing features, the code refactoring is needed in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Appendix\n",
    "\n",
    "$^1$ https://www.wordstream.com/blog/ws/2017/06/29/email-marketing-statistics\n",
    "\n",
    "$^2$ https://www.kaggle.com/veleon/ham-and-spam-dataset\n",
    "\n",
    "$^3$ https://spamassassin.apache.org/\n",
    "\n",
    "$^4$ https://www.kaggle.com/veleon/spam-classification\n",
    "\n",
    "$^5$ https://en.wikipedia.org/wiki/Email_spam\n",
    "\n",
    "$^6$ https://spamassassin.apache.org/old/publiccorpus/readme.html\n",
    "\n",
    "$^7$ https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "$^8$ https://stackoverflow.com/questions/770238/neural-networks-for-email-spam-detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
